#!/usr/bin/env python3
"""
Chain-of-Thought í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤í„°
CoT ì ìš© ì „í›„ ì„±ëŠ¥ ë¹„êµ ë° íš¨ê³¼ ì¸¡ì •
"""

import os
import csv
import json
import time
from datetime import datetime
import google.generativeai as genai
from typing import List, Tuple, Dict, Any
from dataclasses import dataclass
from pathlib import Path
from dotenv import load_dotenv

@dataclass
class TestResult:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    total_samples: int
    correct_predictions: int
    accuracy: float
    predictions: List[str]
    actual_answers: List[str]
    errors: List[Dict[str, Any]]
    parsing_failures: List[str]

class CoTTester:
    """Chain-of-Thought í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥ í…ŒìŠ¤í„°"""
    
    def __init__(self):
        self.setup_gemini()
        self.results_dir = Path("prompt/analysis")
        self.results_dir.mkdir(parents=True, exist_ok=True)
    
    def setup_gemini(self):
        """Gemini API ì„¤ì •"""
        load_dotenv()
        
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            raise ValueError("GEMINI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-2.0-flash-exp')
        print("âœ… Gemini 2.0 Flash ëª¨ë¸ ì„¤ì • ì™„ë£Œ")
        
    def load_prompt(self, prompt_path: str) -> str:
        """í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ"""
        try:
            with open(prompt_path, 'r', encoding='utf-8') as f:
                return f.read().strip()
        except Exception as e:
            self.logger.error(f"í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return ""
    
    def test_prompt_performance(self, prompt_path: str, sample_size: int = 10) -> TestResult:
        """í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print(f"ğŸ§ª í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸ ì‹œì‘: {prompt_path}")
        
        # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
        prompt = self.load_prompt(prompt_path)
        if not prompt:
            print("âŒ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì‹¤íŒ¨")
            return None
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        samples = self._load_samples('data/samples.csv')
        test_samples = samples[:sample_size]
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(test_samples)}")
        
        predictions = []
        actual_answers = []
        errors = []
        parsing_failures = []
        
        # ê°œë³„ ë¬¸ì¥ í…ŒìŠ¤íŠ¸
        for i, (sentence, expected) in enumerate(test_samples):
            try:
                # Gemini API í˜¸ì¶œ
                full_prompt = prompt + f"\n\në‹¤ìŒ ë¬¸ì¥ì„ ë¶„ë¥˜í•˜ì„¸ìš”:\n{sentence}"
                response = self.model.generate_content(full_prompt)
                response_text = response.text.strip()
                
                # ì‘ë‹µ íŒŒì‹±
                predicted = self._parse_response(response_text)
                if not predicted:
                    parsing_failures.append(sentence)
                    predictions.append("íŒŒì‹±ì‹¤íŒ¨")
                    actual_answers.append(expected)
                    print(f"âŒ ìƒ˜í”Œ {i+1}: íŒŒì‹± ì‹¤íŒ¨")
                    continue
                
                predictions.append(predicted)
                actual_answers.append(expected)
                
                # ì •ë‹µê³¼ ë¹„êµ
                is_correct = predicted == expected
                if not is_correct:
                    errors.append({
                        "sentence": sentence,
                        "predicted": predicted,
                        "expected": expected,
                        "response": response_text
                    })
                
                print(f"{'âœ…' if is_correct else 'âŒ'} ìƒ˜í”Œ {i+1}/{len(test_samples)}: {'ì •ë‹µ' if is_correct else 'ì˜¤ë‹µ'}")
                
                # API ì œí•œ ê³ ë ¤ ëŒ€ê¸°
                time.sleep(1)
                
            except Exception as e:
                print(f"âŒ ìƒ˜í”Œ {i+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                parsing_failures.append(sentence)
                predictions.append("íŒŒì‹±ì‹¤íŒ¨")
                actual_answers.append(expected)
        
        # ì •í™•ë„ ê³„ì‚°
        correct_count = sum(1 for p, a in zip(predictions, actual_answers) if p == a and p != "íŒŒì‹±ì‹¤íŒ¨")
        valid_count = len([p for p in predictions if p != "íŒŒì‹±ì‹¤íŒ¨"])
        accuracy = correct_count / valid_count if valid_count > 0 else 0.0
        
        result = TestResult(
            total_samples=len(test_samples),
            correct_predictions=correct_count,
            accuracy=accuracy,
            predictions=predictions,
            actual_answers=actual_answers,
            errors=errors,
            parsing_failures=parsing_failures
        )
        
        print(f"âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ì •í™•ë„: {accuracy:.3f}")
        return result
    
    def _load_samples(self, csv_path: str) -> List[Tuple[str, str]]:
        """CSV íŒŒì¼ì—ì„œ ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ"""
        samples = []
        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if 'user_prompt' in row:
                    sentence = row['user_prompt']
                    answer = row['output']
                else:
                    keys = list(row.keys())
                    sentence = row[keys[0]]
                    answer = row[keys[1]]
                
                samples.append((sentence, answer))
        return samples
    
    def _parse_response(self, response: str) -> str:
        """ì‘ë‹µ íŒŒì‹±"""
        try:
            response = response.strip()
            
            # ì—¬ëŸ¬ ì¤„ì¸ ê²½ìš° ë§ˆì§€ë§‰ ì¤„ ì‚¬ìš©
            lines = response.split('\n')
            for line in reversed(lines):
                line = line.strip()
                if ',' in line and len(line.split(',')) == 4:
                    parts = [part.strip() for part in line.split(',')]
                    if all(parts) and self._is_valid_classification(','.join(parts)):
                        return ','.join(parts)
            
            # ë‹¨ì¼ ì¤„ ì²˜ë¦¬
            if ',' in response and len(response.split(',')) == 4:
                parts = [part.strip() for part in response.split(',')]
                if all(parts) and self._is_valid_classification(','.join(parts)):
                    return ','.join(parts)
            
            return None
            
        except Exception as e:
            print(f"âŒ ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None
    
    def _is_valid_classification(self, text: str) -> bool:
        """ìœ íš¨í•œ ë¶„ë¥˜ ê²°ê³¼ì¸ì§€ í™•ì¸"""
        if not text or "," not in text:
            return False
        
        parts = text.split(",")
        if len(parts) != 4:
            return False
        
        # ê° ë¶€ë¶„ì´ ìœ íš¨í•œ ë¼ë²¨ì¸ì§€ í™•ì¸
        valid_labels = {
            0: ["ì‚¬ì‹¤í˜•", "ì¶”ë¡ í˜•", "ëŒ€í™”í˜•", "ì˜ˆì¸¡í˜•"],
            1: ["ê¸ì •", "ë¶€ì •", "ë¯¸ì •"],
            2: ["ê³¼ê±°", "í˜„ì¬", "ë¯¸ë˜"],
            3: ["í™•ì‹¤", "ë¶ˆí™•ì‹¤"]
        }
        
        for i, part in enumerate(parts):
            if part.strip() not in valid_labels[i]:
                return False
        
        return True
    
    def compare_prompts(self, baseline_path: str, cot_path: str, sample_size: int = 10) -> Dict[str, Any]:
        """CoT ì ìš© ì „í›„ ì„±ëŠ¥ ë¹„êµ"""
        print("ğŸ”„ CoT ì ìš© ì „í›„ ì„±ëŠ¥ ë¹„êµ ì‹œì‘")
        
        # ê¸°ì¤€ í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸
        print("\nğŸ“Š ê¸°ì¤€ í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸:")
        baseline_results = self.test_prompt_performance(baseline_path, sample_size)
        
        # CoT í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸
        print("\nğŸ§  CoT í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸:")
        cot_results = self.test_prompt_performance(cot_path, sample_size)
        
        if not baseline_results or not cot_results:
            return {"error": "í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨"}
        
        # ë¹„êµ ë¶„ì„
        comparison = {
            "baseline": {
                "path": baseline_path,
                "accuracy": baseline_results.accuracy,
                "error_count": len(baseline_results.errors),
                "parsing_failures": len(baseline_results.parsing_failures)
            },
            "cot": {
                "path": cot_path,
                "accuracy": cot_results.accuracy,
                "error_count": len(cot_results.errors),
                "parsing_failures": len(cot_results.parsing_failures)
            },
            "improvement": {
                "accuracy_delta": cot_results.accuracy - baseline_results.accuracy,
                "error_reduction": len(baseline_results.errors) - len(cot_results.errors),
                "parsing_improvement": len(baseline_results.parsing_failures) - len(cot_results.parsing_failures)
            },
            "detailed_baseline": {
                "total_samples": baseline_results.total_samples,
                "correct_predictions": baseline_results.correct_predictions,
                "accuracy": baseline_results.accuracy,
                "errors": baseline_results.errors[:5],  # ì²˜ìŒ 5ê°œë§Œ
                "parsing_failures": baseline_results.parsing_failures[:3]  # ì²˜ìŒ 3ê°œë§Œ
            },
            "detailed_cot": {
                "total_samples": cot_results.total_samples,
                "correct_predictions": cot_results.correct_predictions,
                "accuracy": cot_results.accuracy,
                "errors": cot_results.errors[:5],  # ì²˜ìŒ 5ê°œë§Œ
                "parsing_failures": cot_results.parsing_failures[:3]  # ì²˜ìŒ 3ê°œë§Œ
            }
        }
        
        # ê°œì„  íš¨ê³¼ ë¶„ì„
        accuracy_improvement = comparison["improvement"]["accuracy_delta"]
        if accuracy_improvement > 0:
            comparison["conclusion"] = f"CoT ì ìš©ìœ¼ë¡œ {accuracy_improvement:.3f} ì •í™•ë„ í–¥ìƒ"
        elif accuracy_improvement < 0:
            comparison["conclusion"] = f"CoT ì ìš©ìœ¼ë¡œ {abs(accuracy_improvement):.3f} ì •í™•ë„ í•˜ë½"
        else:
            comparison["conclusion"] = "CoT ì ìš© ì „í›„ ì •í™•ë„ ë™ì¼"
        
        print(f"âœ… ë¹„êµ ì™„ë£Œ: {comparison['conclusion']}")
        return comparison
    
    def save_results(self, results: Dict[str, Any], output_path: str):
        """ê²°ê³¼ ì €ì¥"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")
        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    tester = CoTTester()
    
    # í”„ë¡¬í”„íŠ¸ ê²½ë¡œ ì„¤ì •
    baseline_prompt = "prompt/gemini/enhanced_few_shot_v20250901_175308.txt"
    cot_prompt = "prompt/gemini/enhanced_cot_v1.txt"
    
    # CoT ì ìš© ì „í›„ ì„±ëŠ¥ ë¹„êµ
    comparison_results = tester.compare_prompts(
        baseline_path=baseline_prompt,
        cot_path=cot_prompt,
        sample_size=10
    )
    
    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = f"prompt/analysis/cot_comparison_{timestamp}.json"
    tester.save_results(comparison_results, output_path)
    
    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print("\n=== Chain-of-Thought ì ìš© ê²°ê³¼ ===")
    print(f"ê¸°ì¤€ í”„ë¡¬í”„íŠ¸ ì •í™•ë„: {comparison_results['baseline']['accuracy']:.3f}")
    print(f"CoT í”„ë¡¬í”„íŠ¸ ì •í™•ë„: {comparison_results['cot']['accuracy']:.3f}")
    print(f"ì •í™•ë„ ë³€í™”: {comparison_results['improvement']['accuracy_delta']:+.3f}")
    print(f"ê²°ë¡ : {comparison_results['conclusion']}")
    print(f"\nìƒì„¸ ê²°ê³¼: {output_path}")

if __name__ == "__main__":
    main()