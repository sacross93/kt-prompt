#!/usr/bin/env python3
"""
Gemini 2.5 Flash í…ŒìŠ¤í„° - ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ì˜ ì„±ëŠ¥ì„ ì¸¡ì •
"""

import os
import csv
import json
import time
import google.generativeai as genai
from typing import List, Tuple, Dict, Any
from dataclasses import dataclass
from pathlib import Path

@dataclass
class TestResult:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    total_samples: int
    correct_predictions: int
    accuracy: float
    predictions: List[str]
    actual_answers: List[str]
    errors: List[Dict[str, Any]]
    parsing_failures: List[str]

@dataclass
class ErrorAnalysis:
    """ì˜¤ë¥˜ ë¶„ì„ ê²°ê³¼"""
    total_errors: int
    attribute_errors: Dict[str, int]
    error_patterns: Dict[str, List[str]]
    boundary_cases: List[Dict[str, Any]]
    parsing_failures: List[str]
    confidence_scores: Dict[str, float]

class GeminiFlashTester:
    """Gemini 2.5 Flashë¥¼ ì´ìš©í•œ í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.setup_gemini()
        self.results_dir = Path("prompt/analysis")
        self.results_dir.mkdir(parents=True, exist_ok=True)
    
    def setup_gemini(self):
        """Gemini API ì„¤ì •"""
        # .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ
        from dotenv import load_dotenv
        load_dotenv()
        
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            raise ValueError("GEMINI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-2.0-flash-exp')
        print("âœ… Gemini 2.5 Flash ëª¨ë¸ ì„¤ì • ì™„ë£Œ")
    
    def test_full_dataset(self, prompt: str, sample_size: int = None) -> TestResult:
        """ì „ì²´ ë°ì´í„°ì…‹ ë˜ëŠ” ìƒ˜í”Œë¡œ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰"""
        print(f"ğŸ§ª Gemini 2.5 Flash í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # ë°ì´í„° ë¡œë“œ
        samples = self._load_samples("data/samples.csv")
        
        if sample_size:
            samples = samples[:sample_size]
            print(f"ğŸ“Š ìƒ˜í”Œ í¬ê¸°: {len(samples)}ê°œ")
        else:
            print(f"ğŸ“Š ì „ì²´ ë°ì´í„°ì…‹: {len(samples)}ê°œ")
        
        predictions = []
        actual_answers = []
        errors = []
        parsing_failures = []
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ í…ŒìŠ¤íŠ¸
        batch_size = 5  # API ì œí•œ ê³ ë ¤
        total_batches = (len(samples) + batch_size - 1) // batch_size
        
        for i in range(0, len(samples), batch_size):
            batch = samples[i:i + batch_size]
            batch_num = i // batch_size + 1
            
            print(f"ğŸ”„ ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘...")
            
            try:
                batch_predictions = self._process_batch(prompt, batch)
                
                for j, (sentence, expected) in enumerate(batch):
                    if j < len(batch_predictions):
                        prediction = batch_predictions[j]
                        predictions.append(prediction)
                        actual_answers.append(expected)
                        
                        # ì˜¤ë‹µ ë¶„ì„
                        if prediction != expected:
                            errors.append({
                                "sentence": sentence,
                                "predicted": prediction,
                                "expected": expected,
                                "error_type": self._analyze_error_type(prediction, expected)
                            })
                    else:
                        # íŒŒì‹± ì‹¤íŒ¨
                        parsing_failures.append(sentence)
                        predictions.append("íŒŒì‹±ì‹¤íŒ¨")
                        actual_answers.append(expected)
                
                # API ì œí•œ ê³ ë ¤ ëŒ€ê¸°
                if batch_num < total_batches:
                    time.sleep(2)
                    
            except Exception as e:
                print(f"âŒ ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨í•œ ë°°ì¹˜ëŠ” íŒŒì‹± ì‹¤íŒ¨ë¡œ ì²˜ë¦¬
                for sentence, expected in batch:
                    parsing_failures.append(sentence)
                    predictions.append("íŒŒì‹±ì‹¤íŒ¨")
                    actual_answers.append(expected)
        
        # ì •í™•ë„ ê³„ì‚°
        accuracy = self.calculate_accuracy(predictions, actual_answers)
        
        result = TestResult(
            total_samples=len(samples),
            correct_predictions=sum(1 for p, a in zip(predictions, actual_answers) if p == a),
            accuracy=accuracy,
            predictions=predictions,
            actual_answers=actual_answers,
            errors=errors,
            parsing_failures=parsing_failures
        )
        
        # ê²°ê³¼ ì €ì¥
        self._save_test_result(result)
        
        print(f"âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ: ì •í™•ë„ {accuracy:.1%}")
        return result
    
    def calculate_accuracy(self, predictions: List[str], answers: List[str]) -> float:
        """ì •í™•ë„ ê³„ì‚°"""
        if not predictions or not answers:
            return 0.0
        
        correct = sum(1 for p, a in zip(predictions, answers) if p == a and p != "íŒŒì‹±ì‹¤íŒ¨")
        total = len([p for p in predictions if p != "íŒŒì‹±ì‹¤íŒ¨"])
        
        return correct / total if total > 0 else 0.0
    
    def analyze_errors(self, test_result: TestResult) -> ErrorAnalysis:
        """ìƒì„¸í•œ ì˜¤ë¥˜ ë¶„ì„ ìˆ˜í–‰"""
        print("ğŸ” ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„ ì¤‘...")
        
        # ì†ì„±ë³„ ì˜¤ë¥˜ ì§‘ê³„
        attribute_errors = {"ìœ í˜•": 0, "ê·¹ì„±": 0, "ì‹œì œ": 0, "í™•ì‹¤ì„±": 0}
        error_patterns = {"ìœ í˜•": [], "ê·¹ì„±": [], "ì‹œì œ": [], "í™•ì‹¤ì„±": []}
        boundary_cases = []
        
        for error in test_result.errors:
            predicted = error["predicted"].split(",") if "," in error["predicted"] else []
            expected = error["expected"].split(",") if "," in error["expected"] else []
            
            if len(predicted) == 4 and len(expected) == 4:
                attributes = ["ìœ í˜•", "ê·¹ì„±", "ì‹œì œ", "í™•ì‹¤ì„±"]
                for i, attr in enumerate(attributes):
                    if predicted[i] != expected[i]:
                        attribute_errors[attr] += 1
                        error_patterns[attr].append({
                            "sentence": error["sentence"],
                            "predicted": predicted[i],
                            "expected": expected[i]
                        })
                        
                        # ê²½ê³„ ì‚¬ë¡€ ì‹ë³„
                        if self._is_boundary_case(error["sentence"], predicted[i], expected[i]):
                            boundary_cases.append(error)
        
        # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        confidence_scores = {}
        for attr in attribute_errors:
            total_samples = test_result.total_samples - len(test_result.parsing_failures)
            if total_samples > 0:
                confidence_scores[attr] = 1 - (attribute_errors[attr] / total_samples)
            else:
                confidence_scores[attr] = 0.0
        
        analysis = ErrorAnalysis(
            total_errors=len(test_result.errors),
            attribute_errors=attribute_errors,
            error_patterns=error_patterns,
            boundary_cases=boundary_cases,
            parsing_failures=test_result.parsing_failures,
            confidence_scores=confidence_scores
        )
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        self._save_error_analysis(analysis)
        
        return analysis
    
    def check_target_achievement(self, accuracy: float, target: float = 0.7) -> bool:
        """ëª©í‘œ ì„±ëŠ¥ ë‹¬ì„± ì—¬ë¶€ í™•ì¸"""
        achieved = accuracy >= target
        print(f"ğŸ¯ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€: {achieved} (í˜„ì¬: {accuracy:.1%}, ëª©í‘œ: {target:.1%})")
        return achieved
    
    def _load_samples(self, csv_path: str) -> List[Tuple[str, str]]:
        """CSV íŒŒì¼ì—ì„œ ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ"""
        samples = []
        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for i, row in enumerate(reader):
                if i == 0:  # ì²« ë²ˆì§¸ í–‰ì—ì„œ ì»¬ëŸ¼ëª… í™•ì¸
                    print(f"ğŸ“‹ CSV ì»¬ëŸ¼: {list(row.keys())}")
                
                # ì»¬ëŸ¼ëª… í™•ì¸ í›„ ì ì ˆí•œ í‚¤ ì‚¬ìš©
                if 'user_prompt' in row:
                    sentence = row['user_prompt']
                    answer = row['output']
                elif len(row) >= 2:
                    # ì²« ë²ˆì§¸ì™€ ë‘ ë²ˆì§¸ ì»¬ëŸ¼ ì‚¬ìš©
                    keys = list(row.keys())
                    sentence = row[keys[0]]
                    answer = row[keys[1]]
                else:
                    continue
                
                samples.append((sentence, answer))
        return samples
    
    def _process_batch(self, prompt: str, batch: List[Tuple[str, str]]) -> List[str]:
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¬¸ì¥ë“¤ì„ ì²˜ë¦¬"""
        sentences = [sentence for sentence, _ in batch]
        
        # ë°°ì¹˜ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        batch_prompt = prompt + "\n\në‹¤ìŒ ë¬¸ì¥ë“¤ì„ ë¶„ë¥˜í•˜ì„¸ìš”:\n"
        for i, sentence in enumerate(sentences, 1):
            batch_prompt += f"{i}. {sentence}\n"
        
        try:
            response = self.model.generate_content(batch_prompt)
            response_text = response.text.strip()
            
            # ì‘ë‹µ íŒŒì‹±
            predictions = self._parse_batch_response(response_text, len(sentences))
            return predictions
            
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return ["íŒŒì‹±ì‹¤íŒ¨"] * len(sentences)
    
    def _parse_batch_response(self, response_text: str, expected_count: int) -> List[str]:
        """ë°°ì¹˜ ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ ê°œë³„ ì˜ˆì¸¡ ê²°ê³¼ ì¶”ì¶œ"""
        lines = response_text.strip().split('\n')
        predictions = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # ë²ˆí˜¸ ì œê±° (1. 2. ë“±)
            if line and line[0].isdigit() and '.' in line:
                line = line.split('.', 1)[1].strip()
            
            # ìœ íš¨í•œ ë¶„ë¥˜ ê²°ê³¼ì¸ì§€ í™•ì¸
            if self._is_valid_classification(line):
                predictions.append(line)
        
        # ì˜ˆìƒ ê°œìˆ˜ì™€ ë§ì§€ ì•Šìœ¼ë©´ íŒŒì‹± ì‹¤íŒ¨ë¡œ ì²˜ë¦¬
        while len(predictions) < expected_count:
            predictions.append("íŒŒì‹±ì‹¤íŒ¨")
        
        return predictions[:expected_count]
    
    def _is_valid_classification(self, text: str) -> bool:
        """ìœ íš¨í•œ ë¶„ë¥˜ ê²°ê³¼ì¸ì§€ í™•ì¸"""
        if not text or "," not in text:
            return False
        
        parts = text.split(",")
        if len(parts) != 4:
            return False
        
        # ê° ë¶€ë¶„ì´ ìœ íš¨í•œ ë¼ë²¨ì¸ì§€ í™•ì¸
        valid_labels = {
            0: ["ì‚¬ì‹¤í˜•", "ì¶”ë¡ í˜•", "ëŒ€í™”í˜•", "ì˜ˆì¸¡í˜•"],
            1: ["ê¸ì •", "ë¶€ì •", "ë¯¸ì •"],
            2: ["ê³¼ê±°", "í˜„ì¬", "ë¯¸ë˜"],
            3: ["í™•ì‹¤", "ë¶ˆí™•ì‹¤"]
        }
        
        for i, part in enumerate(parts):
            if part.strip() not in valid_labels[i]:
                return False
        
        return True
    
    def _analyze_error_type(self, predicted: str, expected: str) -> str:
        """ì˜¤ë¥˜ ìœ í˜• ë¶„ì„"""
        if predicted == "íŒŒì‹±ì‹¤íŒ¨":
            return "íŒŒì‹±ì‹¤íŒ¨"
        
        if "," not in predicted or "," not in expected:
            return "í˜•ì‹ì˜¤ë¥˜"
        
        pred_parts = predicted.split(",")
        exp_parts = expected.split(",")
        
        if len(pred_parts) != 4 or len(exp_parts) != 4:
            return "í˜•ì‹ì˜¤ë¥˜"
        
        error_attrs = []
        attributes = ["ìœ í˜•", "ê·¹ì„±", "ì‹œì œ", "í™•ì‹¤ì„±"]
        
        for i, attr in enumerate(attributes):
            if pred_parts[i] != exp_parts[i]:
                error_attrs.append(attr)
        
        return "+".join(error_attrs) if error_attrs else "ê¸°íƒ€"
    
    def _is_boundary_case(self, sentence: str, predicted: str, expected: str) -> bool:
        """ê²½ê³„ ì‚¬ë¡€ì¸ì§€ íŒë‹¨"""
        # íŠ¹ì • í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš° ê²½ê³„ ì‚¬ë¡€ë¡œ íŒë‹¨
        boundary_keywords = [
            "ì•„ì‰¬ì›€", "ë–¨ì–´ì¡Œë‹¤", "ìœ„í—˜", "ë…¼ë¦¬ì…ë‹ˆë‹¤", 
            "ë…¸ë¦¬ëŠ”", "ê²ƒ ê°™ë‹¤", "ì œí’ˆì´ë‹¤", "18ìœ„"
        ]
        
        return any(keyword in sentence for keyword in boundary_keywords)
    
    def _save_test_result(self, result: TestResult):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        output_file = self.results_dir / f"test_results_{timestamp}.json"
        
        result_dict = {
            "total_samples": result.total_samples,
            "correct_predictions": result.correct_predictions,
            "accuracy": result.accuracy,
            "error_count": len(result.errors),
            "parsing_failure_count": len(result.parsing_failures),
            "errors": result.errors[:10],  # ì²˜ìŒ 10ê°œ ì˜¤ë¥˜ë§Œ ì €ì¥
            "parsing_failures": result.parsing_failures[:5]  # ì²˜ìŒ 5ê°œë§Œ ì €ì¥
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result_dict, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: {output_file}")
    
    def _save_error_analysis(self, analysis: ErrorAnalysis):
        """ì˜¤ë¥˜ ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        output_file = self.results_dir / f"error_analysis_{timestamp}.json"
        
        analysis_dict = {
            "total_errors": analysis.total_errors,
            "attribute_errors": analysis.attribute_errors,
            "confidence_scores": analysis.confidence_scores,
            "boundary_cases_count": len(analysis.boundary_cases),
            "parsing_failures_count": len(analysis.parsing_failures),
            "error_patterns_summary": {
                attr: len(patterns) for attr, patterns in analysis.error_patterns.items()
            }
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_dict, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š ì˜¤ë¥˜ ë¶„ì„ ì €ì¥: {output_file}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    tester = GeminiFlashTester()
    
    # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    prompt_file = Path("prompt/gemini/enhanced_v7_improved.txt")
    
    if not prompt_file.exists():
        print("âŒ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    with open(prompt_file, 'r', encoding='utf-8') as f:
        prompt = f.read()
    
    # ì†Œê·œëª¨ í…ŒìŠ¤íŠ¸ (10ê°œ ìƒ˜í”Œ)
    print("ğŸ§ª ì†Œê·œëª¨ í…ŒìŠ¤íŠ¸ ì‹œì‘ (10ê°œ ìƒ˜í”Œ)")
    test_result = tester.test_full_dataset(prompt, sample_size=10)
    
    # ì˜¤ë¥˜ ë¶„ì„
    error_analysis = tester.analyze_errors(test_result)
    
    # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€ í™•ì¸
    target_achieved = tester.check_target_achievement(test_result.accuracy, 0.7)
    
    print("\n" + "="*50)
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("="*50)
    print(f"ì •í™•ë„: {test_result.accuracy:.1%}")
    print(f"ì´ ìƒ˜í”Œ: {test_result.total_samples}ê°œ")
    print(f"ì •ë‹µ: {test_result.correct_predictions}ê°œ")
    print(f"ì˜¤ë‹µ: {len(test_result.errors)}ê°œ")
    print(f"íŒŒì‹± ì‹¤íŒ¨: {len(test_result.parsing_failures)}ê°œ")
    print(f"ëª©í‘œ ë‹¬ì„±: {'âœ…' if target_achieved else 'âŒ'}")
    
    if error_analysis.attribute_errors:
        print("\nğŸ“ˆ ì†ì„±ë³„ ì˜¤ë¥˜:")
        for attr, count in error_analysis.attribute_errors.items():
            print(f"  {attr}: {count}íšŒ ({error_analysis.confidence_scores[attr]:.1%} ì •í™•ë„)")

if __name__ == "__main__":
    main()