"""
GPT-4o ìµœì¢… ê²€ì¦ê¸°
Gemini 2.5 Flashì—ì„œ 0.7ì  ì´ìƒ ë‹¬ì„±í•œ í”„ë¡¬í”„íŠ¸ë¥¼ GPT-4oë¡œ ìµœì¢… ê²€ì¦
"""

import os
import json
import pandas as pd
import openai
from typing import List, Dict, Tuple
from dataclasses import dataclass
import time
import re
from dotenv import load_dotenv

load_dotenv()

@dataclass
class ValidationResult:
    """ê²€ì¦ ê²°ê³¼"""
    model_name: str
    prompt_name: str
    accuracy: float
    total_questions: int
    correct_answers: int
    attribute_accuracies: Dict[str, float]
    improvement_vs_baseline: float
    test_duration: float

class GPT4oValidator:
    """GPT-4oë¥¼ ì´ìš©í•œ ìµœì¢… ì„±ëŠ¥ ê²€ì¦"""
    
    def __init__(self, data_path: str = "data/samples.csv"):
        self.data_path = data_path
        self.api_key = os.getenv('OPENAI_API_KEY')
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        openai.api_key = self.api_key
        self.client = openai.OpenAI(api_key=self.api_key)
        
        # ë°ì´í„° ë¡œë“œ
        self.samples = pd.read_csv(data_path)
        print(f"ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ: {len(self.samples)}ê°œ")
        
        # ê¸°ì¡´ ìµœê³  ì„±ëŠ¥ (ë¹„êµ ê¸°ì¤€)
        self.baseline_performance = 0.7  # system_prompt_final.txt ì„±ëŠ¥
    
    def validate_with_gpt4o(self, prompt_path: str, sample_size: int = 50) -> ValidationResult:
        """GPT-4oë¡œ ìµœì¢… ê²€ì¦"""
        print(f"\n=== GPT-4o ìµœì¢… ê²€ì¦: {os.path.basename(prompt_path)} ===")
        start_time = time.time()
        
        # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
        with open(prompt_path, 'r', encoding='utf-8') as f:
            system_prompt = f.read()
        
        # í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ì„ íƒ
        test_samples = self.samples.head(sample_size)
        sentences = test_samples['user_prompt'].tolist()
        answers = test_samples['output'].tolist()
        
        # GPT-4oë¡œ ë°°ì¹˜ ì²˜ë¦¬
        all_predictions = []
        batch_size = 10
        
        for i in range(0, len(sentences), batch_size):
            batch_sentences = sentences[i:i+batch_size]
            
            # ì…ë ¥ í…ìŠ¤íŠ¸ ìƒì„±
            user_content = ""
            for j, sentence in enumerate(batch_sentences, 1):
                user_content += f"{j}. {sentence}\n"
            
            try:
                # GPT-4o API í˜¸ì¶œ
                response = self.client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_content}
                    ],
                    temperature=0.1,
                    max_tokens=1000
                )
                
                response_text = response.choices[0].message.content
                
                # ê²°ê³¼ íŒŒì‹±
                batch_predictions = self._parse_results(response_text, len(batch_sentences))
                all_predictions.extend(batch_predictions)
                
                print(f"ë°°ì¹˜ {i//batch_size + 1} ì™„ë£Œ ({len(batch_sentences)}ê°œ)")
                
                # API í˜¸ì¶œ ê°„ê²©
                time.sleep(1)
                
            except Exception as e:
                print(f"GPT-4o API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
                # ì˜¤ë¥˜ ì‹œ ë¹ˆ ê²°ê³¼ ì¶”ê°€
                all_predictions.extend([""] * len(batch_sentences))
        
        # ì •í™•ë„ ê³„ì‚°
        accuracy, attribute_accuracies = self._calculate_accuracy(all_predictions, answers)
        
        # ê¸°ì¡´ ì„±ëŠ¥ ëŒ€ë¹„ ê°œì„ ë„ ê³„ì‚°
        improvement = accuracy - self.baseline_performance
        
        test_duration = time.time() - start_time
        
        result = ValidationResult(
            model_name="GPT-4o",
            prompt_name=os.path.basename(prompt_path),
            accuracy=accuracy,
            total_questions=len(sentences),
            correct_answers=int(accuracy * len(sentences)),
            attribute_accuracies=attribute_accuracies,
            improvement_vs_baseline=improvement,
            test_duration=test_duration
        )
        
        print(f"GPT-4o ê²€ì¦ ì™„ë£Œ:")
        print(f"- ì •í™•ë„: {accuracy:.3f} ({result.correct_answers}/{result.total_questions})")
        print(f"- ê¸°ì¡´ ëŒ€ë¹„ ê°œì„ : {improvement:+.3f}")
        print(f"- ì†ì„±ë³„ ì •í™•ë„: {attribute_accuracies}")
        print(f"- ì†Œìš” ì‹œê°„: {test_duration:.1f}ì´ˆ")
        
        return result
    
    def _parse_results(self, response: str, num_questions: int) -> List[str]:
        """ê²°ê³¼ íŒŒì‹±"""
        results = []
        lines = response.strip().split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # "ë²ˆí˜¸. ìœ í˜•,ê·¹ì„±,ì‹œì œ,í™•ì‹¤ì„±" í˜•ì‹ íŒŒì‹±
            match = re.match(r'(\d+)\.\s*([^,]+),([^,]+),([^,]+),([^,\s]+)', line)
            if match:
                _, type_val, polarity, tense, certainty = match.groups()
                result = f"{type_val.strip()},{polarity.strip()},{tense.strip()},{certainty.strip()}"
                results.append(result)
        
        # ê²°ê³¼ ê°œìˆ˜ ë§ì¶”ê¸°
        while len(results) < num_questions:
            results.append("")
        
        return results[:num_questions]
    
    def _calculate_accuracy(self, predictions: List[str], answers: List[str]) -> Tuple[float, Dict[str, float]]:
        """ì •í™•ë„ ê³„ì‚°"""
        total = len(answers)
        correct = 0
        
        # ì†ì„±ë³„ ì •í™•ë„ ê³„ì‚°
        type_correct = polarity_correct = tense_correct = certainty_correct = 0
        
        for pred, ans in zip(predictions, answers):
            if pred == ans:
                correct += 1
            
            # ì†ì„±ë³„ ë¹„êµ
            if pred and ans:
                pred_parts = pred.split(',')
                ans_parts = ans.split(',')
                
                if len(pred_parts) == 4 and len(ans_parts) == 4:
                    if pred_parts[0] == ans_parts[0]:
                        type_correct += 1
                    if pred_parts[1] == ans_parts[1]:
                        polarity_correct += 1
                    if pred_parts[2] == ans_parts[2]:
                        tense_correct += 1
                    if pred_parts[3] == ans_parts[3]:
                        certainty_correct += 1
        
        overall_accuracy = correct / total if total > 0 else 0
        attribute_accuracies = {
            "type": type_correct / total if total > 0 else 0,
            "polarity": polarity_correct / total if total > 0 else 0,
            "tense": tense_correct / total if total > 0 else 0,
            "certainty": certainty_correct / total if total > 0 else 0
        }
        
        return overall_accuracy, attribute_accuracies
    
    def compare_with_baseline(self, result: ValidationResult) -> Dict[str, any]:
        """ê¸°ì¡´ ìµœê³  ì„±ëŠ¥ê³¼ ë¹„êµ"""
        comparison = {
            "current_performance": result.accuracy,
            "baseline_performance": self.baseline_performance,
            "improvement": result.improvement_vs_baseline,
            "improvement_percentage": (result.improvement_vs_baseline / self.baseline_performance) * 100,
            "is_improved": result.improvement_vs_baseline > 0,
            "significance": "significant" if abs(result.improvement_vs_baseline) >= 0.05 else "marginal"
        }
        
        return comparison
    
    def save_validation_result(self, result: ValidationResult, comparison: Dict[str, any]):
        """ê²€ì¦ ê²°ê³¼ ì €ì¥"""
        validation_data = {
            "model": result.model_name,
            "prompt": result.prompt_name,
            "accuracy": result.accuracy,
            "total_questions": result.total_questions,
            "correct_answers": result.correct_answers,
            "attribute_accuracies": result.attribute_accuracies,
            "baseline_comparison": comparison,
            "test_duration": result.test_duration,
            "timestamp": "2024-01-01"
        }
        
        with open("prompt/analysis/gpt4o_validation.json", 'w', encoding='utf-8') as f:
            json.dump(validation_data, f, ensure_ascii=False, indent=2)
        
        print("GPT-4o ê²€ì¦ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def generate_final_report(self, result: ValidationResult, comparison: Dict[str, any]) -> str:
        """ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±"""
        
        status = "ğŸ‰ ì„±ëŠ¥ ê°œì„  ì„±ê³µ!" if comparison["is_improved"] else "ğŸ“Š ì„±ëŠ¥ ìœ ì§€"
        significance = comparison["significance"]
        
        report = f"""
# GPT-4o ìµœì¢… ê²€ì¦ ë¦¬í¬íŠ¸

## ê²€ì¦ ê²°ê³¼
- **ëª¨ë¸**: {result.model_name}
- **í”„ë¡¬í”„íŠ¸**: {result.prompt_name}
- **ì „ì²´ ì •í™•ë„**: {result.accuracy:.3f} ({result.correct_answers}/{result.total_questions})

## ì†ì„±ë³„ ì„±ëŠ¥
- **ìœ í˜• ë¶„ë¥˜**: {result.attribute_accuracies['type']:.3f}
- **ê·¹ì„± ë¶„ë¥˜**: {result.attribute_accuracies['polarity']:.3f}
- **ì‹œì œ ë¶„ë¥˜**: {result.attribute_accuracies['tense']:.3f}
- **í™•ì‹¤ì„± ë¶„ë¥˜**: {result.attribute_accuracies['certainty']:.3f}

## ê¸°ì¡´ ì„±ëŠ¥ ëŒ€ë¹„ ë¹„êµ
- **ê¸°ì¡´ ìµœê³  ì„±ëŠ¥**: {comparison['baseline_performance']:.3f}
- **í˜„ì¬ ì„±ëŠ¥**: {comparison['current_performance']:.3f}
- **ê°œì„ ë„**: {comparison['improvement']:+.3f} ({comparison['improvement_percentage']:+.1f}%)
- **ê°œì„  ìœ ì˜ì„±**: {significance}

## ê²°ë¡ 
{status}

{'ì„±ëŠ¥ì´ ìœ ì˜ë¯¸í•˜ê²Œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.' if comparison['is_improved'] and significance == 'significant' else 'ì¶”ê°€ ìµœì í™”ë¥¼ í†µí•´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.'}

## ê¶Œì¥ì‚¬í•­
{'í˜„ì¬ í”„ë¡¬í”„íŠ¸ë¥¼ ìµœì¢… ë²„ì „ìœ¼ë¡œ ì±„íƒí•˜ê³  ì‹¤ì œ ìš´ì˜ì— ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.' if comparison['is_improved'] else 'ì¶”ê°€ì ì¸ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì„ í†µí•´ ì„±ëŠ¥ ê°œì„ ì„ ì‹œë„í•´ë³´ì„¸ìš”.'}
"""
        
        return report

if __name__ == "__main__":
    try:
        validator = GPT4oValidator()
        
        # ìµœê³  ì„±ëŠ¥ í”„ë¡¬í”„íŠ¸ ê²€ì¦
        best_prompt_path = "prompt/gemini/enhanced_v3.txt"
        
        if os.path.exists(best_prompt_path):
            print("=== GPT-4o ìµœì¢… ê²€ì¦ ì‹œì‘ ===")
            result = validator.validate_with_gpt4o(best_prompt_path, sample_size=30)
            
            # ê¸°ì¡´ ì„±ëŠ¥ê³¼ ë¹„êµ
            comparison = validator.compare_with_baseline(result)
            
            # ê²°ê³¼ ì €ì¥
            validator.save_validation_result(result, comparison)
            
            # ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
            final_report = validator.generate_final_report(result, comparison)
            
            with open("prompt/analysis/final_validation_report.md", 'w', encoding='utf-8') as f:
                f.write(final_report)
            
            print("\n" + final_report)
            
        else:
            print(f"í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {best_prompt_path}")
    
    except Exception as e:
        print(f"GPT-4o ê²€ì¦ ì˜¤ë¥˜: {e}")
        print("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")