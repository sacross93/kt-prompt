#!/usr/bin/env python3
"""
ê²°ê³¼ ë¶„ì„ê¸° - í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ê°œì„  ë°©í–¥ì„ ë„ì¶œ
"""

import json
import glob
from typing import Dict, List, Any
from dataclasses import dataclass
from pathlib import Path

@dataclass
class ImprovementPlan:
    """ê°œì„  ê³„íš"""
    priority_areas: List[str]
    specific_fixes: List[str]
    next_strategies: List[str]
    expected_improvement: float

class ResultAnalyzer:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ê°œì„  ë°©í–¥ì„ ë„ì¶œí•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.analysis_dir = Path("prompt/analysis")
    
    def analyze_latest_results(self) -> Dict[str, Any]:
        """ìµœì‹  í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ë¶„ì„"""
        print("ğŸ“Š ìµœì‹  í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ì¤‘...")
        
        # ìµœì‹  í…ŒìŠ¤íŠ¸ ê²°ê³¼ íŒŒì¼ ì°¾ê¸°
        test_files = list(self.analysis_dir.glob("test_results_*.json"))
        error_files = list(self.analysis_dir.glob("error_analysis_*.json"))
        
        if not test_files or not error_files:
            print("âŒ ë¶„ì„í•  í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        # ìµœì‹  íŒŒì¼ ì„ íƒ
        latest_test = max(test_files, key=lambda x: x.stat().st_mtime)
        latest_error = max(error_files, key=lambda x: x.stat().st_mtime)
        
        # ê²°ê³¼ ë¡œë“œ
        with open(latest_test, 'r', encoding='utf-8') as f:
            test_data = json.load(f)
        
        with open(latest_error, 'r', encoding='utf-8') as f:
            error_data = json.load(f)
        
        # ì¢…í•© ë¶„ì„
        analysis = self._perform_comprehensive_analysis(test_data, error_data)
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        self._save_analysis_report(analysis)
        
        return analysis
    
    def generate_improvement_plan(self, analysis: Dict[str, Any]) -> ImprovementPlan:
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì„  ê³„íš ìƒì„±"""
        print("ğŸ¯ ê°œì„  ê³„íš ìƒì„± ì¤‘...")
        
        current_accuracy = analysis.get("accuracy", 0.0)
        attribute_errors = analysis.get("attribute_errors", {})
        
        # ìš°ì„ ìˆœìœ„ ì˜ì—­ ê²°ì • (ì˜¤ë¥˜ê°€ ë§ì€ ìˆœì„œ)
        priority_areas = []
        sorted_errors = sorted(attribute_errors.items(), key=lambda x: x[1], reverse=True)
        
        for attr, count in sorted_errors:
            if count > 0:
                priority_areas.append(f"{attr} ë¶„ë¥˜ ê°œì„  ({count}íšŒ ì˜¤ë¥˜)")
        
        # êµ¬ì²´ì  ìˆ˜ì •ì‚¬í•­
        specific_fixes = self._generate_specific_fixes(attribute_errors, analysis.get("errors", []))
        
        # ë‹¤ìŒ ì „ëµ
        next_strategies = self._suggest_next_strategies(current_accuracy, attribute_errors)
        
        # ì˜ˆìƒ ê°œì„ ë„
        expected_improvement = self._estimate_improvement(current_accuracy, attribute_errors)
        
        plan = ImprovementPlan(
            priority_areas=priority_areas,
            specific_fixes=specific_fixes,
            next_strategies=next_strategies,
            expected_improvement=expected_improvement
        )
        
        # ê°œì„  ê³„íš ì €ì¥
        self._save_improvement_plan(plan)
        
        return plan
    
    def _perform_comprehensive_analysis(self, test_data: Dict, error_data: Dict) -> Dict[str, Any]:
        """ì¢…í•©ì ì¸ ë¶„ì„ ìˆ˜í–‰"""
        analysis = {
            "accuracy": test_data.get("accuracy", 0.0),
            "total_samples": test_data.get("total_samples", 0),
            "correct_predictions": test_data.get("correct_predictions", 0),
            "error_count": test_data.get("error_count", 0),
            "parsing_failure_count": test_data.get("parsing_failure_count", 0),
            "attribute_errors": error_data.get("attribute_errors", {}),
            "confidence_scores": error_data.get("confidence_scores", {}),
            "errors": test_data.get("errors", [])
        }
        
        # ì„±ëŠ¥ í‰ê°€
        analysis["performance_level"] = self._evaluate_performance_level(analysis["accuracy"])
        
        # ì£¼ìš” ë¬¸ì œì  ì‹ë³„
        analysis["main_issues"] = self._identify_main_issues(analysis)
        
        # ê°œì„  ê°€ëŠ¥ì„± í‰ê°€
        analysis["improvement_potential"] = self._assess_improvement_potential(analysis)
        
        return analysis
    
    def _generate_specific_fixes(self, attribute_errors: Dict[str, int], errors: List[Dict]) -> List[str]:
        """êµ¬ì²´ì ì¸ ìˆ˜ì •ì‚¬í•­ ìƒì„±"""
        fixes = []
        
        # ì†ì„±ë³„ ì˜¤ë¥˜ì— ë”°ë¥¸ ìˆ˜ì •ì‚¬í•­
        if attribute_errors.get("ê·¹ì„±", 0) > 0:
            fixes.append("ë¶€ì •ì  ìƒí™©ì˜ ì‚¬ì‹¤ ì„œìˆ  = ê¸ì • ê·¹ì„± ê·œì¹™ ê°•í™”")
            fixes.append("ëª…í™•í•œ ë¶€ì • í‘œí˜„ ì˜ˆì‹œ ì¶”ê°€")
        
        if attribute_errors.get("ìœ í˜•", 0) > 0:
            fixes.append("ê°œì¸ ê²½í—˜ì˜ ê°ê´€ì  ì„œìˆ  = ì‚¬ì‹¤í˜• ê·œì¹™ ëª…í™•í™”")
            fixes.append("ì¶”ë¡ í˜• vs ì‚¬ì‹¤í˜• ê²½ê³„ ì‚¬ë¡€ ì˜ˆì‹œ ë³´ê°•")
        
        if attribute_errors.get("ì‹œì œ", 0) > 0:
            fixes.append("í˜„ì¬ ìƒíƒœ ì„¤ëª… = í˜„ì¬ ì‹œì œ ê·œì¹™ ê°•ì¡°")
            fixes.append("ì‹œì œ íŒë‹¨ ê¸°ì¤€ ì„¸ë¶„í™”")
        
        if attribute_errors.get("í™•ì‹¤ì„±", 0) > 0:
            fixes.append("í™•ì‹¤ì„± íŒë‹¨ ê¸°ì¤€ ëª…í™•í™”")
            fixes.append("ì¶”ì¸¡ í‘œí˜„ ì‹ë³„ ê·œì¹™ ê°•í™”")
        
        # ì‹¤ì œ ì˜¤ë¥˜ ì‚¬ë¡€ ê¸°ë°˜ ìˆ˜ì •ì‚¬í•­
        for error in errors[:3]:  # ì²˜ìŒ 3ê°œ ì˜¤ë¥˜ë§Œ ë¶„ì„
            sentence = error.get("sentence", "")
            if "ì•„ì‰¬ì›€" in sentence:
                fixes.append("ê°ì • í‘œí˜„ì˜ ê°ê´€ì  ì„œìˆ  ì²˜ë¦¬ ê·œì¹™ ì¶”ê°€")
            elif "ë–¨ì–´ì¡Œë‹¤" in sentence:
                fixes.append("ë¶€ì •ì  ê²°ê³¼ì˜ ì‚¬ì‹¤ ì„œìˆ  ì˜ˆì‹œ ê°•í™”")
        
        return list(set(fixes))  # ì¤‘ë³µ ì œê±°
    
    def _suggest_next_strategies(self, accuracy: float, attribute_errors: Dict[str, int]) -> List[str]:
        """ë‹¤ìŒ ì „ëµ ì œì•ˆ"""
        strategies = []
        
        if accuracy < 0.75:
            strategies.append("Few-shot learning ì˜ˆì‹œ í™•ì¥")
            strategies.append("ê²½ê³„ ì‚¬ë¡€ ì¤‘ì‹¬ ê·œì¹™ ì„¸ë¶„í™”")
        
        if accuracy < 0.8:
            strategies.append("Chain-of-Thought ì¶”ë¡  ê³¼ì • ë„ì…")
            strategies.append("ì†ì„±ë³„ ìš°ì„ ìˆœìœ„ ì¬ì¡°ì •")
        
        # ê°€ì¥ ë¬¸ì œê°€ ë˜ëŠ” ì†ì„±ì— ë”°ë¥¸ ì „ëµ
        max_error_attr = max(attribute_errors.items(), key=lambda x: x[1])[0] if attribute_errors else None
        
        if max_error_attr == "ê·¹ì„±":
            strategies.append("ê·¹ì„± ë¶„ë¥˜ ì „ìš© ì˜ˆì‹œ ì§‘ì¤‘ ì¶”ê°€")
        elif max_error_attr == "ìœ í˜•":
            strategies.append("ìœ í˜• ë¶„ë¥˜ ê²°ì • íŠ¸ë¦¬ ë°©ì‹ ë„ì…")
        elif max_error_attr == "ì‹œì œ":
            strategies.append("ì‹œì œ í‘œí˜„ íŒ¨í„´ ë§¤ì¹­ ê°•í™”")
        
        return strategies
    
    def _estimate_improvement(self, current_accuracy: float, attribute_errors: Dict[str, int]) -> float:
        """ì˜ˆìƒ ê°œì„ ë„ ì¶”ì •"""
        total_errors = sum(attribute_errors.values())
        
        if total_errors == 0:
            return current_accuracy
        
        # ê° ì˜¤ë¥˜ë¥¼ 50% ê°œì„ í•œë‹¤ê³  ê°€ì •
        potential_fixes = total_errors * 0.5
        total_samples = 10  # í˜„ì¬ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜
        
        improvement = potential_fixes / total_samples
        estimated_accuracy = min(current_accuracy + improvement, 1.0)
        
        return estimated_accuracy
    
    def _evaluate_performance_level(self, accuracy: float) -> str:
        """ì„±ëŠ¥ ìˆ˜ì¤€ í‰ê°€"""
        if accuracy >= 0.85:
            return "ìš°ìˆ˜"
        elif accuracy >= 0.75:
            return "ì–‘í˜¸"
        elif accuracy >= 0.65:
            return "ë³´í†µ"
        else:
            return "ê°œì„ í•„ìš”"
    
    def _identify_main_issues(self, analysis: Dict) -> List[str]:
        """ì£¼ìš” ë¬¸ì œì  ì‹ë³„"""
        issues = []
        
        accuracy = analysis["accuracy"]
        attribute_errors = analysis["attribute_errors"]
        parsing_failures = analysis["parsing_failure_count"]
        
        if accuracy < 0.7:
            issues.append("ì „ì²´ ì •í™•ë„ê°€ ëª©í‘œì¹˜ ë¯¸ë‹¬")
        
        if parsing_failures > 0:
            issues.append("ì¶œë ¥ í˜•ì‹ íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ")
        
        # ì†ì„±ë³„ ë¬¸ì œì 
        for attr, count in attribute_errors.items():
            if count > 1:
                issues.append(f"{attr} ë¶„ë¥˜ì—ì„œ ë°˜ë³µì  ì˜¤ë¥˜")
        
        return issues
    
    def _assess_improvement_potential(self, analysis: Dict) -> str:
        """ê°œì„  ê°€ëŠ¥ì„± í‰ê°€"""
        accuracy = analysis["accuracy"]
        total_errors = sum(analysis["attribute_errors"].values())
        
        if accuracy >= 0.8:
            return "ë¯¸ì„¸ ì¡°ì • ë‹¨ê³„"
        elif total_errors <= 3:
            return "ë†’ì€ ê°œì„  ê°€ëŠ¥ì„±"
        elif total_errors <= 5:
            return "ì¤‘ê°„ ê°œì„  ê°€ëŠ¥ì„±"
        else:
            return "ëŒ€í­ ê°œì„  í•„ìš”"
    
    def _save_analysis_report(self, analysis: Dict[str, Any]):
        """ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥"""
        from datetime import datetime
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = self.analysis_dir / f"comprehensive_analysis_{timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ ì¢…í•© ë¶„ì„ ì €ì¥: {output_file}")
    
    def _save_improvement_plan(self, plan: ImprovementPlan):
        """ê°œì„  ê³„íš ì €ì¥"""
        from datetime import datetime
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = self.analysis_dir / f"improvement_plan_{timestamp}.json"
        
        plan_dict = {
            "priority_areas": plan.priority_areas,
            "specific_fixes": plan.specific_fixes,
            "next_strategies": plan.next_strategies,
            "expected_improvement": plan.expected_improvement
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(plan_dict, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ¯ ê°œì„  ê³„íš ì €ì¥: {output_file}")
    
    def generate_summary_report(self, analysis: Dict[str, Any], plan: ImprovementPlan) -> str:
        """ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = f"""
# í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥ ë¶„ì„ ë° ê°œì„  ê³„íš

## í˜„ì¬ ì„±ëŠ¥
- **ì •í™•ë„**: {analysis['accuracy']:.1%}
- **ì„±ëŠ¥ ìˆ˜ì¤€**: {analysis['performance_level']}
- **ì´ ìƒ˜í”Œ**: {analysis['total_samples']}ê°œ
- **ì •ë‹µ**: {analysis['correct_predictions']}ê°œ
- **ì˜¤ë‹µ**: {analysis['error_count']}ê°œ

## ì†ì„±ë³„ ì„±ëŠ¥
"""
        
        for attr, score in analysis.get('confidence_scores', {}).items():
            error_count = analysis.get('attribute_errors', {}).get(attr, 0)
            report += f"- **{attr}**: {score:.1%} ì •í™•ë„ ({error_count}íšŒ ì˜¤ë¥˜)\n"
        
        report += f"""
## ì£¼ìš” ë¬¸ì œì 
"""
        for issue in analysis.get('main_issues', []):
            report += f"- {issue}\n"
        
        report += f"""
## ê°œì„  ê³„íš

### ìš°ì„ ìˆœìœ„ ì˜ì—­
"""
        for area in plan.priority_areas:
            report += f"- {area}\n"
        
        report += f"""
### êµ¬ì²´ì  ìˆ˜ì •ì‚¬í•­
"""
        for fix in plan.specific_fixes:
            report += f"- {fix}\n"
        
        report += f"""
### ë‹¤ìŒ ì „ëµ
"""
        for strategy in plan.next_strategies:
            report += f"- {strategy}\n"
        
        report += f"""
## ì˜ˆìƒ ì„±ê³¼
- **ëª©í‘œ ì •í™•ë„**: {plan.expected_improvement:.1%}
- **ê°œì„  ê°€ëŠ¥ì„±**: {analysis['improvement_potential']}

## ë‹¤ìŒ ë‹¨ê³„
1. ìš°ì„ ìˆœìœ„ ì˜ì—­ë¶€í„° í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
2. ì†Œê·œëª¨ í…ŒìŠ¤íŠ¸ë¡œ ê°œì„  íš¨ê³¼ í™•ì¸
3. ì ì§„ì ìœ¼ë¡œ ìƒ˜í”Œ í¬ê¸° í™•ëŒ€
4. ëª©í‘œ ë‹¬ì„±ê¹Œì§€ ë°˜ë³µ ê°œì„ 
"""
        
        return report

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    analyzer = ResultAnalyzer()
    
    # ìµœì‹  ê²°ê³¼ ë¶„ì„
    analysis = analyzer.analyze_latest_results()
    
    if not analysis:
        print("âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê°œì„  ê³„íš ìƒì„±
    improvement_plan = analyzer.generate_improvement_plan(analysis)
    
    # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
    summary_report = analyzer.generate_summary_report(analysis, improvement_plan)
    
    # ë¦¬í¬íŠ¸ ì €ì¥
    report_file = analyzer.analysis_dir / "latest_analysis_report.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(summary_report)
    
    print("\n" + "="*60)
    print("ğŸ“Š ê²°ê³¼ ë¶„ì„ ì™„ë£Œ")
    print("="*60)
    print(f"í˜„ì¬ ì •í™•ë„: {analysis['accuracy']:.1%}")
    print(f"ì„±ëŠ¥ ìˆ˜ì¤€: {analysis['performance_level']}")
    print(f"ì˜ˆìƒ ê°œì„ ë„: {improvement_plan.expected_improvement:.1%}")
    print(f"ê°œì„  ê°€ëŠ¥ì„±: {analysis['improvement_potential']}")
    print(f"\nğŸ“„ ìƒì„¸ ë¦¬í¬íŠ¸: {report_file}")
    
    # ì£¼ìš” ê°œì„ ì‚¬í•­ ì¶œë ¥
    if improvement_plan.priority_areas:
        print(f"\nğŸ¯ ìš°ì„  ê°œì„  ì˜ì—­:")
        for area in improvement_plan.priority_areas[:3]:
            print(f"  - {area}")

if __name__ == "__main__":
    main()