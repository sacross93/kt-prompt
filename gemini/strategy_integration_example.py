"""
StrategyManagerì™€ ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•© ì˜ˆì œ
ì‹¤ì œ ìµœì í™” íŒŒì´í”„ë¼ì¸ì—ì„œ StrategyManagerë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
"""

import sys
import os
from datetime import datetime

sys.path.append('.')
sys.path.append('models')

from strategy_manager import StrategyManager, StrategyResult
from advanced_generator import AdvancedPromptGenerator, PromptStrategy

class OptimizationPipeline:
    """ìµœì í™” íŒŒì´í”„ë¼ì¸ í†µí•© ì˜ˆì œ"""
    
    def __init__(self):
        self.strategy_manager = StrategyManager()
        self.prompt_generator = AdvancedPromptGenerator()
        self.target_accuracy = 0.7
        self.max_iterations = 10
        
    def run_optimization_cycle(self):
        """ì „ì²´ ìµœì í™” ì‚¬ì´í´ ì‹¤í–‰"""
        print("=== í”„ë¡¬í”„íŠ¸ ìµœì í™” íŒŒì´í”„ë¼ì¸ ì‹œì‘ ===\n")
        
        iteration = 0
        while iteration < self.max_iterations:
            iteration += 1
            print(f"--- ë°˜ë³µ {iteration} ---")
            
            # 1. ë‹¤ìŒ ì „ëµ ì„ íƒ
            next_strategy = self.strategy_manager.select_next_strategy(
                self.strategy_manager.optimization_history
            )
            print(f"ì„ íƒëœ ì „ëµ: {next_strategy.value}")
            
            # 2. ì „ëµì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.prompt_generator.create_enhanced_prompt(next_strategy)
            prompt_version = f"v{iteration}_{next_strategy.value}"
            
            # 3. ì‹œë®¬ë ˆì´ì…˜ëœ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì‹¤ì œë¡œëŠ” Gemini API í˜¸ì¶œ)
            test_result = self._simulate_test(next_strategy, iteration)
            
            # 4. ê²°ê³¼ ê¸°ë¡
            strategy_result = StrategyResult(
                strategy=next_strategy,
                accuracy=test_result['accuracy'],
                execution_time=test_result['execution_time'],
                error_count=test_result['error_count'],
                parsing_failures=test_result['parsing_failures'],
                timestamp=datetime.now().isoformat(),
                prompt_version=prompt_version,
                detailed_metrics=test_result['detailed_metrics']
            )
            
            self.strategy_manager.record_strategy_result(strategy_result)
            
            print(f"ê²°ê³¼: ì •í™•ë„ {test_result['accuracy']:.3f}, "
                  f"íš¨ê³¼ì„± {strategy_result.get_effectiveness_score():.3f}")
            
            # 5. ëª©í‘œ ë‹¬ì„± í™•ì¸
            if test_result['accuracy'] >= self.target_accuracy:
                print(f"ğŸ‰ ëª©í‘œ ì •í™•ë„ {self.target_accuracy} ë‹¬ì„±!")
                break
            
            # 6. ìˆ˜ë ´ í™•ì¸
            if self.strategy_manager.optimization_history.is_converged():
                print("âš ï¸ ì„±ëŠ¥ ìˆ˜ë ´ ê°ì§€. íƒìƒ‰ì  ì „ëµìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            
            print()
        
        # ìµœì¢… ê²°ê³¼
        self._print_final_results()
        
    def _simulate_test(self, strategy: PromptStrategy, iteration: int):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” Gemini API í˜¸ì¶œ)"""
        # ì „ëµë³„ ê¸°ë³¸ ì„±ëŠ¥ (ì‹œë®¬ë ˆì´ì…˜)
        base_performance = {
            PromptStrategy.BASELINE: 0.65,
            PromptStrategy.EXPLICIT_RULES: 0.68,
            PromptStrategy.FEW_SHOT: 0.71,
            PromptStrategy.CHAIN_OF_THOUGHT: 0.69,
            PromptStrategy.HYBRID: 0.74
        }
        
        # ë°˜ë³µì— ë”°ë¥¸ ê°œì„  (í•™ìŠµ íš¨ê³¼)
        improvement = min(0.05, iteration * 0.01)
        base_acc = base_performance.get(strategy, 0.65)
        accuracy = min(0.85, base_acc + improvement)
        
        # ë…¸ì´ì¦ˆ ì¶”ê°€ (ì‹¤ì œ í…ŒìŠ¤íŠ¸ì˜ ë³€ë™ì„±)
        import random
        noise = random.uniform(-0.02, 0.02)
        accuracy = max(0.5, min(0.9, accuracy + noise))
        
        return {
            'accuracy': accuracy,
            'execution_time': random.uniform(20, 60),
            'error_count': int(100 * (1 - accuracy)),
            'parsing_failures': random.randint(0, 3),
            'detailed_metrics': {
                'type_accuracy': accuracy + random.uniform(-0.05, 0.05),
                'polarity_accuracy': accuracy + random.uniform(-0.05, 0.05),
                'tense_accuracy': accuracy + random.uniform(-0.05, 0.05),
                'certainty_accuracy': accuracy + random.uniform(-0.05, 0.05)
            }
        }
    
    def _print_final_results(self):
        """ìµœì¢… ê²°ê³¼ ì¶œë ¥"""
        print("=== ìµœì í™” ì™„ë£Œ ===")
        
        history = self.strategy_manager.optimization_history
        print(f"ìµœê³  ì„±ëŠ¥: {history.best_score:.3f}")
        print(f"ìµœì  ì „ëµ: {history.best_strategy.value if history.best_strategy else 'None'}")
        print(f"ì´ ê°œì„  íšŸìˆ˜: {history.total_improvements}")
        print(f"ì´ ë°˜ë³µ íšŸìˆ˜: {len(history.iterations)}")
        
        # ì „ëµë³„ ì„±ëŠ¥ ìš”ì•½
        print("\nì „ëµë³„ ì„±ëŠ¥ ìš”ì•½:")
        comparison = self.strategy_manager.get_strategy_comparison()
        for strategy, effectiveness in comparison["strategy_rankings"]:
            usage_stats = comparison["usage_statistics"].get(strategy, {})
            print(f"  {strategy}: íš¨ê³¼ì„± {effectiveness:.3f}, "
                  f"í‰ê·  ì •í™•ë„ {usage_stats.get('avg_accuracy', 0):.3f}")
        
        # ê¶Œì¥ì‚¬í•­
        print(f"\nê¶Œì¥ì‚¬í•­:")
        if history.best_score >= self.target_accuracy:
            print("- ëª©í‘œ ì„±ëŠ¥ ë‹¬ì„±! GPT-4o ìµœì¢… ê²€ì¦ì„ ì§„í–‰í•˜ì„¸ìš”.")
        else:
            next_strategy = self.strategy_manager.select_next_strategy(history)
            print(f"- ë‹¤ìŒ ì‹œë„ ê¶Œì¥ ì „ëµ: {next_strategy.value}")
        
        # ë°ì´í„° ì €ì¥
        self.strategy_manager.save_strategy_data()
        print("\nì „ëµ ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def demonstrate_strategy_selection_logic():
    """ì „ëµ ì„ íƒ ë¡œì§ ì‹œì—°"""
    print("=== ì „ëµ ì„ íƒ ë¡œì§ ì‹œì—° ===\n")
    
    manager = StrategyManager()
    
    # ì‹œë‚˜ë¦¬ì˜¤ë³„ ì „ëµ ì„ íƒ í…ŒìŠ¤íŠ¸
    scenarios = [
        {
            "name": "ì´ˆê¸° ìƒíƒœ",
            "history": manager.optimization_history,
            "description": "ì•„ë¬´ ì „ëµë„ ì‹œë„í•˜ì§€ ì•Šì€ ìƒíƒœ"
        },
        {
            "name": "ì„±ëŠ¥ ê°œì„  ì¤‘",
            "history": type(manager.optimization_history)(
                iterations=[
                    {"accuracy": 0.65, "strategy": "baseline"},
                    {"accuracy": 0.68, "strategy": "explicit_rules"},
                    {"accuracy": 0.71, "strategy": "few_shot"}
                ],
                best_score=0.71,
                best_strategy="few_shot",
                strategy_effectiveness={"baseline": 0.65, "explicit_rules": 0.68, "few_shot": 0.71},
                convergence_status="improving",
                total_improvements=2
            ),
            "description": "ì§€ì†ì ìœ¼ë¡œ ì„±ëŠ¥ì´ ê°œì„ ë˜ê³  ìˆëŠ” ìƒíƒœ"
        },
        {
            "name": "ì„±ëŠ¥ ì •ì²´",
            "history": type(manager.optimization_history)(
                iterations=[
                    {"accuracy": 0.70, "strategy": "baseline"},
                    {"accuracy": 0.701, "strategy": "explicit_rules"},
                    {"accuracy": 0.699, "strategy": "few_shot"},
                    {"accuracy": 0.700, "strategy": "cot"}
                ],
                best_score=0.701,
                best_strategy="explicit_rules",
                strategy_effectiveness={"baseline": 0.70, "explicit_rules": 0.701, "few_shot": 0.699, "cot": 0.700},
                convergence_status="stagnant",
                total_improvements=1
            ),
            "description": "ì„±ëŠ¥ ê°œì„ ì´ ì •ì²´ëœ ìƒíƒœ"
        }
    ]
    
    for scenario in scenarios:
        print(f"ì‹œë‚˜ë¦¬ì˜¤: {scenario['name']}")
        print(f"ì„¤ëª…: {scenario['description']}")
        
        selected_strategy = manager.select_next_strategy(scenario['history'])
        print(f"ì„ íƒëœ ì „ëµ: {selected_strategy.value}")
        
        # ì „ëµ ì„ íƒ ì´ìœ  ì„¤ëª…
        if not scenario['history'].strategy_effectiveness:
            print("ì´ìœ : ì´ˆê¸° ìƒíƒœì´ë¯€ë¡œ ê¸°ë³¸ ì „ëµ ì„ íƒ")
        elif scenario['history'].is_converged():
            print("ì´ìœ : ìˆ˜ë ´ ìƒíƒœ ê°ì§€, íƒìƒ‰ì  ì „ëµ ì„ íƒ")
        else:
            best_strategy = max(scenario['history'].strategy_effectiveness.items(), key=lambda x: x[1])
            print(f"ì´ìœ : í˜„ì¬ ìµœê³  ì„±ëŠ¥ ì „ëµ ({best_strategy[0]}: {best_strategy[1]:.3f}) ê¸°ë°˜ ì„ íƒ")
        
        print()

def demonstrate_combination_optimization():
    """ì „ëµ ì¡°í•© ìµœì í™” ì‹œì—°"""
    print("=== ì „ëµ ì¡°í•© ìµœì í™” ì‹œì—° ===\n")
    
    manager = StrategyManager()
    
    # ë‹¤ì–‘í•œ ì¡°í•© ì‹œë‚˜ë¦¬ì˜¤
    combination_scenarios = [
        {
            "strategies": [PromptStrategy.FEW_SHOT],
            "description": "ë‹¨ì¼ ì „ëµ"
        },
        {
            "strategies": [PromptStrategy.FEW_SHOT, PromptStrategy.EXPLICIT_RULES],
            "description": "ì´ì¤‘ ì¡°í•©"
        },
        {
            "strategies": [PromptStrategy.FEW_SHOT, PromptStrategy.CHAIN_OF_THOUGHT, PromptStrategy.EXPLICIT_RULES],
            "description": "ì‚¼ì¤‘ ì¡°í•©"
        }
    ]
    
    for scenario in combination_scenarios:
        print(f"ì¡°í•© ì‹œë‚˜ë¦¬ì˜¤: {scenario['description']}")
        print(f"ì „ëµë“¤: {[s.value for s in scenario['strategies']]}")
        
        optimal_combo = manager.find_optimal_combination(scenario['strategies'])
        print(f"ìµœì  ì¡°í•©: {optimal_combo.combo_name}")
        print(f"ì˜ˆìƒ ì‹œë„ˆì§€: {optimal_combo.expected_synergy:.2f}")
        print(f"ì„¤ëª…: {optimal_combo.description}")
        print()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("StrategyManager í†µí•© ì‹œìŠ¤í…œ ë°ëª¨\n")
    
    # 1. ì „ëµ ì„ íƒ ë¡œì§ ì‹œì—°
    demonstrate_strategy_selection_logic()
    
    # 2. ì „ëµ ì¡°í•© ìµœì í™” ì‹œì—°
    demonstrate_combination_optimization()
    
    # 3. ì „ì²´ ìµœì í™” íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = OptimizationPipeline()
    pipeline.run_optimization_cycle()

if __name__ == "__main__":
    main()