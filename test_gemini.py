import pandas as pd
import google.generativeai as genai
import os
from dotenv import load_dotenv
import time
import random
import re

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
gemini_api_key = os.getenv('GEMINI_API_KEY')

def load_system_prompt(file_path):
    """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read()

def test_classification_gemini(system_prompt, test_sentences, model="gemini-2.5-flash-preview-05-20", temperature=0.4):
    """Geminië¥¼ ì‚¬ìš©í•œ ë¬¸ì¥ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸"""
    # API í‚¤ ì„¤ì •
    genai.configure(api_key=gemini_api_key)
    
    # ì…ë ¥ í˜•ì‹ ìƒì„±
    user_input = ""
    for i, sentence in enumerate(test_sentences, 1):
        user_input += f"{i}. {sentence}\n"
    
    try:
        # ëª¨ë¸ ì´ˆê¸°í™”
        model_instance = genai.GenerativeModel(
            model_name=model,
            generation_config={
                "temperature": temperature,
                "max_output_tokens": 1000,
            }
        )
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ì‚¬ìš©ì ì…ë ¥ ê²°í•©
        full_prompt = f"{system_prompt}\n\n{user_input.strip()}"
        
        response = model_instance.generate_content(full_prompt)
        
        return response.text.strip()
        
    except Exception as e:
        return f"Error: {str(e)}"

def calculate_accuracy(predictions, ground_truth):
    """ë¶„ë¥˜ ì •í™•ë„ ê³„ì‚°"""
    if len(predictions) != len(ground_truth):
        print(f"ê¸¸ì´ ë¶ˆì¼ì¹˜: ì˜ˆì¸¡ {len(predictions)}, ì •ë‹µ {len(ground_truth)}")
        return 0.0, [0, 0, 0, 0]
    
    total_correct = 0
    total_attributes = 0
    attribute_correct = [0, 0, 0, 0]  # ìœ í˜•, ê·¹ì„±, ì‹œì œ, í™•ì‹¤ì„±
    
    for pred, truth in zip(predictions, ground_truth):
        pred_parts = pred.split(',')
        truth_parts = truth.split(',')
        
        if len(pred_parts) == 4 and len(truth_parts) == 4:
            for i, (p, t) in enumerate(zip(pred_parts, truth_parts)):
                if p.strip() == t.strip():
                    total_correct += 1
                    attribute_correct[i] += 1
                total_attributes += 1
    
    overall_accuracy = total_correct / total_attributes if total_attributes > 0 else 0.0
    
    # ì†ì„±ë³„ ì •í™•ë„
    attr_names = ['ìœ í˜•', 'ê·¹ì„±', 'ì‹œì œ', 'í™•ì‹¤ì„±']
    attr_accuracies = []
    for i, correct in enumerate(attribute_correct):
        acc = correct / len(predictions) if len(predictions) > 0 else 0.0
        attr_accuracies.append(acc)
        print(f"{attr_names[i]} ì •í™•ë„: {acc:.1%}")
    
    return overall_accuracy, attr_accuracies

def calculate_korean_ratio(text):
    """í•œê¸€ ë¬¸ì ë¹„ìœ¨ ê³„ì‚°"""
    korean_chars = 0
    total_chars = 0
    
    for char in text:
        if char not in [' ', '\n', '\t']:
            total_chars += 1
            if 'ê°€' <= char <= 'í£':
                korean_chars += 1
    
    return korean_chars / total_chars if total_chars > 0 else 0.0

def calculate_competition_score(accuracy, korean_ratio, length):
    """ëŒ€íšŒ ì ìˆ˜ ê³„ì‚° (ì¶”ì •)"""
    # ê¸¸ì´ ì ìˆ˜ ì¶”ì • (935ì ê¸°ì¤€ìœ¼ë¡œ ìµœì í™”)
    if 800 <= length <= 1100:
        length_score = 1.0
    elif length < 800:
        length_score = length / 800
    else:
        length_score = 1100 / length
    
    total_score = 0.8 * accuracy + 0.1 * korean_ratio + 0.1 * length_score
    return total_score

def run_gemini_test(system_prompt_path, sample_size=30, temperature=0.4):
    """Gemini í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv('data/samples.csv')
    
    # ëœë¤ ìƒ˜í”Œ ì„ íƒ
    sample_df = df.sample(n=sample_size, random_state=42)
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    system_prompt = load_system_prompt(system_prompt_path)
    
    print(f"=== Gemini 2.5 Flash ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸ ({sample_size}ê°œ ìƒ˜í”Œ) ===")
    print(f"ëª¨ë¸: gemini-2.5-flash-preview-05-20")
    print(f"Temperature: {temperature}")
    print(f"í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(system_prompt)}ì")
    print(f"í•œê¸€ ë¹„ìœ¨: {calculate_korean_ratio(system_prompt):.1%}")
    print()
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_sentences = sample_df['user_prompt'].tolist()
    ground_truth = sample_df['output'].tolist()
    
    print("í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    start_time = time.time()
    result = test_classification_gemini(system_prompt, test_sentences, temperature=temperature)
    end_time = time.time()
    
    print(f"ì‘ë‹µ ì‹œê°„: {end_time - start_time:.1f}ì´ˆ")
    print()
    
    print("=== ëª¨ë¸ ì¶œë ¥ ===")
    print(result)
    print()
    
    # ê²°ê³¼ íŒŒì‹± ë° ì •í™•ë„ ê³„ì‚°
    try:
        predictions = []
        for line in result.split('\n'):
            line = line.strip()
            # "1. ì‚¬ì‹¤í˜•,ê¸ì •,í˜„ì¬,í™•ì‹¤" í˜•ì‹ ì²˜ë¦¬
            if line and re.match(r'^\d+\.', line):
                # ë²ˆí˜¸ì™€ ì  ë‹¤ìŒì˜ ë‚´ìš© ì¶”ì¶œ
                parts = re.split(r'^\d+\.?\s*', line, 1)
                if len(parts) == 2:
                    pred = parts[1].strip()
                    predictions.append(pred)
        
        print(f"=== ì„±ëŠ¥ í‰ê°€ ===")
        print(f"ì˜ˆì¸¡ ê°œìˆ˜: {len(predictions)}")
        print(f"ì‹¤ì œ ê°œìˆ˜: {len(ground_truth)}")
        
        if len(predictions) > 0:
            accuracy, attr_accuracies = calculate_accuracy(predictions, ground_truth)
            print(f"ì „ì²´ ë¶„ë¥˜ ì •í™•ë„: {accuracy:.1%}")
            
            # ëŒ€íšŒ ì ìˆ˜ ê³„ì‚°
            korean_ratio = calculate_korean_ratio(system_prompt)
            competition_score = calculate_competition_score(accuracy, korean_ratio, len(system_prompt))
            print(f"ì˜ˆìƒ ëŒ€íšŒ ì ìˆ˜: {competition_score:.4f}")
            
            # ìƒì„¸ ë¹„êµ (ì²˜ìŒ 10ê°œë§Œ)
            print("\n=== ìƒì„¸ ë¹„êµ (ì²˜ìŒ 10ê°œ) ===")
            for i in range(min(10, len(predictions), len(ground_truth))):
                print(f"{i+1}. {test_sentences[i]}")
                print(f"   ì˜ˆì¸¡: {predictions[i] if i < len(predictions) else 'N/A'}")
                print(f"   ì •ë‹µ: {ground_truth[i]}")
                
                # ì¼ì¹˜ ì—¬ë¶€ í™•ì¸
                if i < len(predictions):
                    pred_parts = predictions[i].split(',')
                    truth_parts = ground_truth[i].split(',')
                    if len(pred_parts) == 4 and len(truth_parts) == 4:
                        matches = [p.strip() == t.strip() for p, t in zip(pred_parts, truth_parts)]
                        match_str = " ".join(["âœ“" if m else "âœ—" for m in matches])
                        print(f"   ì¼ì¹˜: {match_str} (ìœ í˜• ê·¹ì„± ì‹œì œ í™•ì‹¤ì„±)")
                print()
            
            return accuracy, attr_accuracies, competition_score
            
    except Exception as e:
        print(f"ê²°ê³¼ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
        return 0.0, [0, 0, 0, 0], 0.0

def test_multiple_prompts(prompt_files, sample_size=30):
    """ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ë¥¼ Geminië¡œ í…ŒìŠ¤íŠ¸í•˜ì—¬ ìµœì  í”„ë¡¬í”„íŠ¸ ì°¾ê¸°"""
    results = {}
    
    for prompt_file in prompt_files:
        print(f"\n{'='*60}")
        print(f"í…ŒìŠ¤íŠ¸: {prompt_file}")
        print('='*60)
        
        try:
            accuracy, attr_accuracies, competition_score = run_gemini_test(prompt_file, sample_size=sample_size, temperature=0.4)
            results[prompt_file] = {
                'accuracy': accuracy,
                'competition_score': competition_score,
                'type_acc': attr_accuracies[0],
                'polarity_acc': attr_accuracies[1],
                'tense_acc': attr_accuracies[2],
                'certainty_acc': attr_accuracies[3]
            }
        except Exception as e:
            print(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            continue
    
    # ê²°ê³¼ ë¹„êµ
    if len(results) > 1:
        print(f"\n{'='*80}")
        print("=== í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥ ë¹„êµ (Gemini 2.5 Flash ê¸°ì¤€) ===")
        print(f"{'í”„ë¡¬í”„íŠ¸':<30} {'ì „ì²´ì •í™•ë„':<12} {'ëŒ€íšŒì ìˆ˜':<12} {'ê·¹ì„±ì •í™•ë„':<12} {'ì‹œì œì •í™•ë„':<12}")
        print("-" * 90)
        
        sorted_results = sorted(results.items(), key=lambda x: x[1]['competition_score'], reverse=True)
        for prompt_file, result in sorted_results:
            name = prompt_file.replace('system_prompt_', '').replace('.txt', '')
            print(f"{name:<30} {result['accuracy']:<12.1%} {result['competition_score']:<12.4f} "
                  f"{result['polarity_acc']:<12.1%} {result['tense_acc']:<12.1%}")
        
        best_prompt = sorted_results[0][0]
        best_score = sorted_results[0][1]['competition_score']
        print(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {best_prompt} (ì ìˆ˜: {best_score:.4f})")
        print(f"ğŸ’¡ ì´ í”„ë¡¬í”„íŠ¸ë¥¼ GPT-4oë¡œ ìµœì¢… ì œì¶œí•˜ì„¸ìš”!")
    
    return results

if __name__ == "__main__":
    # ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸ë¡œ ìµœì  í”„ë¡¬í”„íŠ¸ ì°¾ê¸°
    prompt_files = [
        'system_prompt_v1.txt',
        'system_prompt_v1_enhanced.txt',
        'system_prompt_v2.txt',
        'system_prompt_optimized.txt',
        'system_prompt_best.txt'
    ]
    
    print("ğŸ”® Gemini 2.5 Flashë¡œ í”„ë¡¬í”„íŠ¸ ìµœì í™” í…ŒìŠ¤íŠ¸")
    print("ğŸ’¡ ëª©í‘œ: GPT-4o ì œì¶œìš© ìµœê³  í”„ë¡¬í”„íŠ¸ ì°¾ê¸°")
    
    results = test_multiple_prompts(prompt_files, sample_size=30)